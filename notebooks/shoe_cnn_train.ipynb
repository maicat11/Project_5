{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shoe Training via Same CNN Pipeline as Clothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# %load_ext autoreload\n",
    "# %reload_ext autoreload\n",
    "\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "\n",
    "# import necessary keras modules\n",
    "from keras.preprocessing import image \n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping  \n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras.models import Sequential\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "# custom functions\n",
    "from layer_output import get_dense_layers, path_to_tensor, paths_to_tensor\n",
    "from loading import load_files       \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # define function to load train, test, and validation datasets\n",
    "# def load_dataset(path):\n",
    "#     data = load_files(path, ignore_files='.DS_Store')\n",
    "#     shoe_files = np.array(data['filenames'])\n",
    "#     shoe_targets = np_utils.to_categorical(np.array(data['target']), 9)\n",
    "#     return shoe_files, shoe_targets\n",
    "\n",
    "# # load train, test, and validation datasets\n",
    "# train_files, train_targets = load_dataset('../data/shoes/train')\n",
    "# valid_files, valid_targets = load_dataset('../data/shoes/validate')\n",
    "# test_files, test_targets = load_dataset('../data/shoes/test')\n",
    "\n",
    "# # load list of clothing names\n",
    "# shoe_names = [item[20:-1] for item in sorted(glob(\"../data/shoes/train/*/\"))]\n",
    "\n",
    "# # print info about the dataset\n",
    "# print(f'There are {len(shoe_names)} total shoe categories.')\n",
    "# print(f'There are {len(np.hstack([train_files, valid_files, test_files]))} total shoe images.\\n')\n",
    "# print(f'There are {len(train_files)} training shoe images.')\n",
    "# print(f'There are {len(valid_files)} validation shoe images.')\n",
    "# print(f'There are {len(test_files)} test shoe images.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>path</th>\n",
       "      <th>shoe_type</th>\n",
       "      <th>color</th>\n",
       "      <th>class_11</th>\n",
       "      <th>class_12</th>\n",
       "      <th>class_13</th>\n",
       "      <th>class_14</th>\n",
       "      <th>class_15</th>\n",
       "      <th>class_16</th>\n",
       "      <th>...</th>\n",
       "      <th>class_75</th>\n",
       "      <th>class_76</th>\n",
       "      <th>class_77</th>\n",
       "      <th>class_81</th>\n",
       "      <th>class_82</th>\n",
       "      <th>class_83</th>\n",
       "      <th>class_84</th>\n",
       "      <th>class_85</th>\n",
       "      <th>class_86</th>\n",
       "      <th>class_87</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>validate</td>\n",
       "      <td>../data/shoes/validate/calf_boots/7838888.6.jpg</td>\n",
       "      <td>calf_boots</td>\n",
       "      <td>grape</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>validate</td>\n",
       "      <td>../data/shoes/validate/calf_boots/7677053.325.jpg</td>\n",
       "      <td>calf_boots</td>\n",
       "      <td>charcoal</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>validate</td>\n",
       "      <td>../data/shoes/validate/calf_boots/8024575.6357...</td>\n",
       "      <td>calf_boots</td>\n",
       "      <td>charcoal</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>validate</td>\n",
       "      <td>../data/shoes/validate/calf_boots/8075982.278.jpg</td>\n",
       "      <td>calf_boots</td>\n",
       "      <td>brownish</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>validate</td>\n",
       "      <td>../data/shoes/validate/calf_boots/8036333.84.jpg</td>\n",
       "      <td>calf_boots</td>\n",
       "      <td>charcoal</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 60 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       type                                               path   shoe_type  \\\n",
       "0  validate    ../data/shoes/validate/calf_boots/7838888.6.jpg  calf_boots   \n",
       "1  validate  ../data/shoes/validate/calf_boots/7677053.325.jpg  calf_boots   \n",
       "2  validate  ../data/shoes/validate/calf_boots/8024575.6357...  calf_boots   \n",
       "3  validate  ../data/shoes/validate/calf_boots/8075982.278.jpg  calf_boots   \n",
       "4  validate   ../data/shoes/validate/calf_boots/8036333.84.jpg  calf_boots   \n",
       "\n",
       "      color  class_11  class_12  class_13  class_14  class_15  class_16  ...  \\\n",
       "0     grape         0         0         0         0         0         0  ...   \n",
       "1  charcoal         0         0         0         0         0         0  ...   \n",
       "2  charcoal         0         0         0         0         0         0  ...   \n",
       "3  brownish         0         0         0         0         0         0  ...   \n",
       "4  charcoal         0         0         0         0         0         0  ...   \n",
       "\n",
       "   class_75  class_76  class_77  class_81  class_82  class_83  class_84  \\\n",
       "0         0         0         0         0         0         0         0   \n",
       "1         0         0         0         0         0         0         0   \n",
       "2         0         0         0         0         0         0         0   \n",
       "3         0         0         0         0         0         0         0   \n",
       "4         0         0         0         0         0         0         0   \n",
       "\n",
       "   class_85  class_86  class_87  \n",
       "0         0         0         0  \n",
       "1         0         0         0  \n",
       "2         0         0         0  \n",
       "3         0         0         0  \n",
       "4         0         0         0  \n",
       "\n",
       "[5 rows x 60 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags = pd.read_pickle('saved_models/shoe_labels_df.pickle')\n",
    "tags.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_files = np.array(tags[tags['type']=='train']['path'])\n",
    "valid_files = np.array(tags[tags['type']=='validate']['path'])\n",
    "test_files = np.array(tags[tags['type']=='test']['path'])\n",
    "\n",
    "train_targets = np.array(tags[tags['type']=='train'].loc[:,'class_11':]).astype('float32')\n",
    "valid_targets =np.array( tags[tags['type']=='validate'].loc[:,'class_11':]).astype('float32')\n",
    "test_targets = np.array(tags[tags['type']=='test'].loc[:,'class_11':]).astype('float32')\n",
    "valid_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18038/18038 [00:22<00:00, 815.44it/s]\n",
      "100%|██████████| 2164/2164 [00:03<00:00, 648.56it/s]\n",
      "100%|██████████| 2013/2013 [00:02<00:00, 728.31it/s]\n"
     ]
    }
   ],
   "source": [
    "from PIL import ImageFile                            \n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True                 \n",
    "\n",
    "# pre-process the data for Keras\n",
    "# rescale the images by dividing every pixel in every image by 255.\n",
    "train_tensors = paths_to_tensor(train_files).astype('float32')/255\n",
    "valid_tensors = paths_to_tensor(valid_files).astype('float32')/255\n",
    "test_tensors = paths_to_tensor(test_files).astype('float32')/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the CNN Architechture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "batch_normalization_2 (Batch (None, 224, 224, 3)       12        \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 110, 110, 16)      1216      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 55, 55, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 26, 26, 32)        12832     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 6, 6, 64)          8256      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 3, 3, 64)          0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_2 ( (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "vectors (Dense)              (None, 300)               19500     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 56)                16856     \n",
      "=================================================================\n",
      "Total params: 58,672\n",
      "Trainable params: 58,666\n",
      "Non-trainable params: 6\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# Define the architecture.\n",
    "model.add(BatchNormalization(input_shape=(224, 224, 3)))\n",
    "model.add(Conv2D(filters=16, kernel_size=5, strides=2, padding='valid', activation='relu', \n",
    "                 input_shape=(224, 224, 3)))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Conv2D(filters=32, kernel_size=5, strides=2, padding='valid', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Dropout(rate=0.3))\n",
    "model.add(Conv2D(filters=64, kernel_size=2, strides=2, padding='valid', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(GlobalAveragePooling2D())\n",
    "\n",
    "model.add(Dense(units=300, activation='relu', name='vectors')) # extract vectors from here and cluster\n",
    "model.add(Dropout(rate=0.3))\n",
    "model.add(Dense(56, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and configure augmented image generator\n",
    "datagen_train = ImageDataGenerator(\n",
    "    width_shift_range=0.1,  # randomly shift images horizontally (10% of total width)\n",
    "    height_shift_range=0.1,  # randomly shift images vertically (10% of total height)\n",
    "    horizontal_flip=True) # randomly flip images horizontally\n",
    "\n",
    "datagen_train.fit(train_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 18038 samples, validate on 2164 samples\n",
      "Epoch 1/20\n",
      "18038/18038 [==============================] - 1673s 93ms/step - loss: 2.4906 - acc: 0.2669 - val_loss: 1.7728 - val_acc: 0.4552\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.77283, saving model to saved_models/weights.best.from_scratch_shoes2.hdf5\n",
      "Epoch 2/20\n",
      "18038/18038 [==============================] - 1625s 90ms/step - loss: 1.7734 - acc: 0.4316 - val_loss: 1.6768 - val_acc: 0.4635\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.77283 to 1.67677, saving model to saved_models/weights.best.from_scratch_shoes2.hdf5\n",
      "Epoch 3/20\n",
      "18038/18038 [==============================] - 1631s 90ms/step - loss: 1.5468 - acc: 0.4956 - val_loss: 1.3620 - val_acc: 0.5684\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.67677 to 1.36200, saving model to saved_models/weights.best.from_scratch_shoes2.hdf5\n",
      "Epoch 4/20\n",
      "18038/18038 [==============================] - 1707s 95ms/step - loss: 1.4204 - acc: 0.5334 - val_loss: 1.2335 - val_acc: 0.5957\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.36200 to 1.23346, saving model to saved_models/weights.best.from_scratch_shoes2.hdf5\n",
      "Epoch 5/20\n",
      "18038/18038 [==============================] - 1753s 97ms/step - loss: 1.3378 - acc: 0.5554 - val_loss: 1.2329 - val_acc: 0.5846\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.23346 to 1.23292, saving model to saved_models/weights.best.from_scratch_shoes2.hdf5\n",
      "Epoch 6/20\n",
      "18038/18038 [==============================] - 1699s 94ms/step - loss: 1.2613 - acc: 0.5709 - val_loss: 1.0604 - val_acc: 0.6354\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.23292 to 1.06036, saving model to saved_models/weights.best.from_scratch_shoes2.hdf5\n",
      "Epoch 7/20\n",
      "18038/18038 [==============================] - 1718s 95ms/step - loss: 1.2051 - acc: 0.5882 - val_loss: 1.0584 - val_acc: 0.6340\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.06036 to 1.05838, saving model to saved_models/weights.best.from_scratch_shoes2.hdf5\n",
      "Epoch 8/20\n",
      "18038/18038 [==============================] - 1721s 95ms/step - loss: 1.1694 - acc: 0.5980 - val_loss: 1.0333 - val_acc: 0.6446\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.05838 to 1.03335, saving model to saved_models/weights.best.from_scratch_shoes2.hdf5\n",
      "Epoch 9/20\n",
      "18038/18038 [==============================] - 1744s 97ms/step - loss: 1.1424 - acc: 0.6088 - val_loss: 1.0349 - val_acc: 0.6340\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 1.03335\n",
      "Epoch 10/20\n",
      "18038/18038 [==============================] - 1692s 94ms/step - loss: 1.1012 - acc: 0.6168 - val_loss: 0.9263 - val_acc: 0.6594\n",
      "\n",
      "Epoch 00010: val_loss improved from 1.03335 to 0.92633, saving model to saved_models/weights.best.from_scratch_shoes2.hdf5\n",
      "Epoch 11/20\n",
      " 9024/18038 [==============>...............] - ETA: 13:56 - loss: 1.0912 - acc: 0.6284"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-ad1c3bf9be49>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m history = model.fit(train_tensors, train_targets, \n\u001b[1;32m      8\u001b[0m           \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_targets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m           epochs=epochs, batch_size=32, callbacks=[checkpointer, early_stop], verbose=1)\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/project5_2/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/envs/project5_2/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/project5_2/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/project5_2/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/project5_2/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1380\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1381\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1382\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1384\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.from_scratch_shoes2.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "history = model.fit(train_tensors, train_targets, \n",
    "          validation_data=(valid_tensors, valid_targets),\n",
    "          epochs=epochs, batch_size=32, callbacks=[checkpointer, early_stop], verbose=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph of training/validation Accuracy and Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(acc) + 1)\n",
    "plt.figure(figsize=(16,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc', c='r')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss', c='r')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get index of predicted item for each image in test set\n",
    "shoe_predictions = [np.argmax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\n",
    "\n",
    "# report test accuracy\n",
    "test_accuracy = 100*np.sum(np.array(shoe_predictions)==np.argmax(test_targets, axis=1))/len(shoe_predictions)\n",
    "print('Test accuracy: {}'.format(round(test_accuracy, 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://towardsdatascience.com/visualizing-intermediate-activation-in-convolutional\n",
    "# -neural-networks-with-keras-260b36d60d0\n",
    "model.save('saved_models/shoes_cnn_color.h5')\n",
    "# load the model with best validation loss\n",
    "model.load_weights('saved_models/weights.best.from_scratch_shoes_color.hdf5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = '../data/shoes/test/sandals/7910998.332551.jpg'\n",
    "img_tensor = path_to_tensor(img_path)\n",
    "img_tensor /= 255.\n",
    "plt.imshow(img_tensor[0])\n",
    "plt.axis('off')\n",
    "plt.show();\n",
    "\n",
    "image = np.vstack([img_tensor])\n",
    "classes = model.predict_classes(image)\n",
    "print(\"Predicted class:\", shoe_names[int(classes)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get a matrix of all images Dense layer stacked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_files[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# all_shoe_paths = np.append(train_files, np.append(valid_files, test_files))\n",
    "\n",
    "# dense_layers, dense_df = get_dense_layers(model, all_shoe_paths)\n",
    "# # pickle.dump(dense_df, open(\"saved_models/dense_shoe_df2.pickle\", \"wb\" ))\n",
    "# print(dense_layers.shape)\n",
    "# print(dense_df.shape)\n",
    "# dense_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means cluster graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "import pylab as pl\n",
    "\n",
    "X = dense_layers\n",
    "pca = PCA(n_components=2).fit(X)\n",
    "pca_2d = pca.transform(X)\n",
    "kmeans = KMeans(n_clusters=9, random_state=11)\n",
    "kmeans.fit(X)\n",
    "pl.figure()\n",
    "pl.title('K-means with 9 clusters')\n",
    "pl.scatter(pca_2d[:, 0], pca_2d[:, 1], c=kmeans.labels_, cmap='tab20');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T-SNE Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://datascience.stackexchange.com/questions/31700/how-to-print-kmeans-cluster-python\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Embed the features into 2 features using TSNE\n",
    "X_embedded_tsne = TSNE(n_components=2, perplexity=25).fit_transform(X)\n",
    "\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.scatter(X_embedded_tsne[:,0], X_embedded_tsne[:,1], c = kmeans.labels_, cmap='tab20')\n",
    "plt.title('TSNE of Shoe Targets')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.colorbar()\n",
    "plt.show();\n",
    "# plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Assign each file to a cluster**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign each training file to a cluster for understanding the clusters and troubleshooting\n",
    "for cluster_num in range(9):\n",
    "    mask = np.where(kmeans.labels_ == cluster_num)[0]\n",
    "    for idx in mask:\n",
    "        print(f\"Image {all_clothing_paths[idx]} is in cluster: {cluster_num}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of images in each cluster\n",
    "[np.where(kmeans.labels_ == cluster_num, 1, 0).sum() for cluster_num in range(9)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Confusion matrix of labels and clusters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets1 = np.array([np.where(target == 1)[0][0] for target in train_targets])\n",
    "targets2 = np.array([np.where(target == 1)[0][0] for target in valid_targets])\n",
    "targets3 = np.array([np.where(target == 1)[0][0] for target in test_targets])\n",
    "targets = np.append(targets1, np.append(targets2, targets3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a confusion matrix out of the the labels and clusters\n",
    "confusion = dict()\n",
    "\n",
    "for target, label in zip(targets, kmeans.labels_):\n",
    "    confusion[(target, label)] = confusion.get((target, label), 0) + 1\n",
    "\n",
    "for target in range(9):\n",
    "    line = '  '.join([f'{confusion.get((target,label),0):4d}' for label in range(9)])\n",
    "    print(line)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, name in enumerate(shoe_names):\n",
    "    print(i,': ', name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fixed_kernel",
   "language": "python",
   "name": "project5_2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
